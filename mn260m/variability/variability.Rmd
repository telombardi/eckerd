---
title: "Measures of Variability"
runningheader: "Measures of Variability" # only for pdf output
subtitle: "Measuring Dispersion" # only for html output
output:
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_html: default
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
library('plyr')
library(epiDisplay)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

In previous discussions, we introduced the concept of measures of location. They
help us understand the typical values in a distribution. While measures of 
location are certainly helpful, they are even more so when paired with a 
measure of variability.

# Variance

```{marginfigure}
**Sample Variance**: 
\Large
\begin{equation} 
    s^2 = \frac{\Sigma (x_i - \bar{x})^2}{n-1}
\end{equation}
where $\Sigma$ means the sum of, $x_i$ is the value of x for the ith observation,
$\bar{x}$ is the sample mean, and $n$ is the number of observations in the sample data set.
```

The variance is based on the difference between the value of each observation
$x_i$ and the mean. The difference between each $x_i$ and the mean is called
a **deviation about the mean**. These deviations about the mean are squared 
in the calculation of the variance. The term $\Sigma (x_i - \bar{x})^2$  is 
often referred to as the **sum of squares** or **sum of squared deviations**.
Before walking through an example, let's
compare the equations for sample variance and population variance. \

There are three major differences between these two equations:
1. $s^2$ denotes the sample variance; $\sigma^2$ denotes the population variance.
2. The sample variance relies on the sample mean ($\bar{x}$), while the 
population variance uses the population mean ($\mu$).
3. The sample variance divides the sum of squared deviations by n-1, while the the
population variance divides the sum of squares by n. 

```{marginfigure}
**Point Estimator**:
A sample statistic, such $\bar{x}$, $s^2$, and $s$, used to estimate the 
corresponding population parameter.
```

In order to understand these differences, we need to review the difference
between samples and populations, and sample statistics and a population parameters.
Often, we want to know the population parameter, but we cannot directly measure it
due to cost, ethical or practical considerations. Statistical inference gives us
a way to infer population parameters from the statistics of well-chosen samples.
**Point estimation** is the process of using a sample statistic like the 
sample mean ($\bar{x}$), sample variance ($s^2$), or sample standard deviation
($s$) to estimate the corresponding population parameter. For example, we use
the sample mean, $\bar{x}$, in the sample variance formula because we very 
likely do not know or cannot know the population mean, $\mu$. Also, we divide
the sum of squared deviations by n-1, rather than N, because this produces an
unbiased (better) estimation of the population variance. In short, the 
sample variance estimates the population variance and where the sample formula
departs from its population equivalent, it does so to improve its estimation.

```{marginfigure}
**Population Variance**: 
\Large
\begin{equation} 
    \sigma^2 = \frac{\Sigma (x_i - \mu)^2}{N}
\end{equation}
where $\Sigma$ means the sum of, $x_i$ is the value of x for the ith observation,
$\mu$ is the population mean, and $N$ is the number of observations in the population data set.
```

Let's walk through the sample variance formula.\
\
**$s^2$** is the symbol for sample variance.

**$\Sigma (x_i - \bar{x})^2$** represents the sum of squared deviations (sum of squares).\
\
**$n-1$** is the size of the sample less one.\
\
Let's calculate the arithmetic mean for the following sample.\
\
**46, 54, 42, 46, 32**\

## Step 1: Calculate the sum of squared deviations

Recall that the sample mean ($\bar{x}$) of these data is 44. 

$_i$|$x_i$|$\bar{x}$|$(x_i - \bar{x})$  |$(x_i - \bar{x})^2$
--|-------|---------|-------------------|---------------------
1 | 46    |    44   |       2           |       4
2 | 54    |    44   |      10           |     100
3 | 42    |    44   |      -2           |       4
4 | 46    |    44   |       2           |       4
5 | 32    |    44   |     -12           |     144


**$(x_i - \bar{x})^2$** = 4 + 100 + 4 + 4 + 144 = **`r sum(4,100,4,4,144)`**

## Step 2: Divide by the number of observations minus 1.

The number of observations is 5.\
\
**$n-1$** = 4\
\
Now that we know the two components of the formula,
$s^2 = \frac{\Sigma (x_i - \bar{x})^2}{n-1}$, we only need to divide them to arrive
at our answer.\
\
**$s^2$**= 256/4 = `r var(c(46,54,42,46,32))`\
\
The sample variance (**$s^2$**) in this example is 64.\

# Standard Deviation

The **median** is another measure of location frequently used. The median is the
middle value in a sorted data set. For an **odd number of observations**, the median
is the middle number. For example, given the sample data in our previous 
example, the median is:

## Step 1: Sort the Data

**46, 54, 42, 46, 32** becomes ordered data set **32, 42, 46, 46, 54**

$_i$|$x_i$
-|-----
1| 32
2| 42
3| 46
4| 46
5| 54

## Step 2: Find the Middle Value

The middle value (the 3rd value of 5) is 46. The median is 46. 
You will notice that the median equals the mean calculated
in our previous example for these data.

## Calculating the Median with an Even Number of Observations

When calculating the median with an **even number of observations**, first sort
the data. Given the following sorted data set, we find the two middle values
and calculate their mean to find the median.

**32, 42, 44, 46, 46, 54**\
\
(44 + 46)/2 = 45 is the median in this example.

# Comparing the Mean and the Median

```{r, echo = FALSE, fig.margin=TRUE, fig.height=4, fig.cap="Normal Distribution"}
 N <- 100000
 x <- rnorm(n=N, mean=100, sd=10)
 hist(x, 
 xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1, 
   col='lightblue', xlab=' ', ylab=' ', axes=F,
   main='Symmetic Distribution')
#lines(density(x,bw=1), col='red', lwd=3)
abline(v = mean(x),                     # Add line for mean
       col = "green",
       lwd = 3)
abline(v = median(x),                    # Add line for median
       col = "red",
       lwd = 3)
```

In distributions that are symmetric, we expect that the mean and median will 
be extremely close if not equal. In the example shown in Figure 1, the mean
and median are 100.03 and 100.04 respectively. The values are so close that 
their lines overlap in Figure 1.

```{marginfigure}
**Outlier**:
An unusually small or unusually large data value.
```

This is not always the case, however. Consider the example offered by Figure 2.
We can see the mean and median pulling apart. In fact, the median (the red line)
is closer to the typical value in this distribution. The mean is pulled toward
the extreme values in this distribution because the mean as a measurement is 
sensitive to **outliers**, unusual values in a distibution. The median, on the
other hand, is relatively insensitive to outliers. Often the median is the 
measure of central tendency reported with economic or business data because
measurements related to money are often skewed like the data shown in Figure 2.

```{r, echo = FALSE, fig.margin=TRUE, fig.height=4, fig.cap="Skewed Distribution"}
 N <- 100000
 #x <- rnbinom(N, 10, .3)
 x<-  rexp(N,0.15)
 hist(x, 
 xlim=c(min(x),max(x)), probability=T, nclass=max(x)-min(x)+1, 
   col='lightblue', xlab=' ', ylab=' ', axes=F,
   main='Positive Skew')
#lines(density(x,bw=1), col='red', lwd=3)
abline(v = mean(x),                     # Add line for mean
       col = "green",
       lwd = 3)
abline(v = median(x),                    # Add line for median
       col = "red",
       lwd = 3)
```

# Mode

```{marginfigure}
**Mode**:
The value that occurs with the greatest frequency.
```

In our running problem, **46, 54, 42, 46, 32**, the mode is 46 as it is the
only value that occurs twice in the data set.

# Measures of Location and Scales of Measurement

In our previous sessions, we introduced scales of measurement to help us
understand when certain techniques are appropriate given the properties of our
data. Scales of Measurement is particularly useful when thinking about 
reasonable uses of measures of central tendency. Table 1 summarizes the 
measures of location most appropriate for different scales of measurement.

Scale of Measurement|Measure of Location
--------------------|-------------------
Nominal             | Mode
Ordinal             | Median, Mode
Interval            | Arithmetic Mean, Median, Mode
Ratio               | Geometric Mean, Arithmetic Mean, Median, Mode
